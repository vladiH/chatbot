{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model that work along to dialogflow to get more accuracy and precission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"chat.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import os,inspect\n",
    "import re\n",
    "import string\n",
    "import h5py\n",
    "import math as mt\n",
    "from tqdm import tqdm\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os,inspect\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout, Add\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import load_model, Model\n",
    "\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention_decoder_f import attention_LSTM\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root path\n",
    "path_file = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean memory gpu\n",
    "def limit_mem():\n",
    "    K.get_session().close()\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_mem() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory from cornell dataset\n",
    "cornell=os.path.join(path_file,'data/cornell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id2line():\n",
    "    movie = os.path.join(cornell,'movie_lines.txt')\n",
    "    lines=open(movie).read().split('\\n')\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        _line = line.split(' +++$+++ ')\n",
    "        if len(_line) == 5:\n",
    "            id2line[_line[0]] = _line[4]\n",
    "    return id2line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversations():\n",
    "    conversation = os.path.join(cornell,'movie_conversations.txt')\n",
    "    conv_lines = open(conversation).read().split('\\n')\n",
    "    convs = [ ]\n",
    "    for line in conv_lines[:-1]:\n",
    "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        convs.append(_line.split(','))\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conversations(convs,id2line,path=''):\n",
    "    idx = 0\n",
    "    for conv in convs:\n",
    "        f_conv = open(path + str(idx)+'.txt', 'w')\n",
    "        for line_id in conv:\n",
    "            f_conv.write(id2line[line_id])\n",
    "            f_conv.write('\\n')\n",
    "        f_conv.close()\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_dataset(convs, id2line):\n",
    "    questions = []; answers = []\n",
    "\n",
    "    for conv in convs:\n",
    "        if len(conv) %2 != 0:\n",
    "            conv = conv[:-1]\n",
    "        for i in range(len(conv)):\n",
    "            if i%2 == 0:\n",
    "                questions.append(id2line[conv[i]])\n",
    "            else:\n",
    "                answers.append(id2line[conv[i]])\n",
    "\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_seq2seq_files(questions, answers, path='',TESTSET_SIZE = 30000):\n",
    "    train_x = os.path.join(path_file,'processing_data/train.enc')\n",
    "    train_y = os.path.join(path_file,'processing_data/train.dec')\n",
    "    test_x = os.path.join(path_file,'processing_data/test.enc')\n",
    "    test_y = os.path.join(path_file,'processing_data/test.dec')\n",
    "    # open files\n",
    "    train_enc = open(train_x,'w')\n",
    "    train_dec = open(train_y,'w')\n",
    "    test_enc  = open(test_x, 'w')\n",
    "    test_dec  = open(test_y, 'w')\n",
    "\n",
    "    # choose 30,000 (TESTSET_SIZE) items to put into testset\n",
    "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        line = lower_case(questions[i])\n",
    "        line = rid_number(line)\n",
    "        line = rid_space(line)\n",
    "        line = replace_not_dont(line)\n",
    "        line = rid_character(line)\n",
    "        line = \" \".join(line.split())\n",
    "        for x in line.split(\" \"):\n",
    "            if x not in word_to_index:\n",
    "                line = line.replace(x, \"unk\")\n",
    "        linea = lower_case(answers[i])\n",
    "        linea = rid_number(linea)\n",
    "        linea = rid_space(linea)\n",
    "        linea = replace_not_dont(linea)\n",
    "        linea = rid_character(linea)\n",
    "        linea = \" \".join(linea.split())\n",
    "        for x in linea.split(\" \"):\n",
    "            if x not in word_to_index:\n",
    "                linea = linea.replace(x, \"unk\")\n",
    "                \n",
    "        if i in test_ids:\n",
    "            test_enc.write(line+'\\n')\n",
    "            test_dec.write(linea+ '\\n' )\n",
    "        else:\n",
    "            train_enc.write(line+'\\n')\n",
    "            train_dec.write(linea+ '\\n' )\n",
    "        if i%2000 == 0:\n",
    "            print ('written {} lines' .format(str(i)))\n",
    "\n",
    "    # close files\n",
    "    train_enc.close()\n",
    "    train_dec.close()\n",
    "    test_enc.close()\n",
    "    test_dec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = get_id2line()\n",
    "print ('gathered id2line dictionary.\\n')\n",
    "convs = get_conversations()\n",
    "print ('>> gathered conversations.\\n')\n",
    "questions, answers = gather_dataset(convs,id2line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( 'gathered questions and answers.\\n')\n",
    "prepare_seq2seq_files(questions,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize all text into lower case\n",
    "def lower_case(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all number\n",
    "def rid_number(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters\n",
    "def rid_character(text):\n",
    "    return text.translate(str.maketrans('—’¹“‘”', '      ', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove white space\n",
    "def rid_space(text):\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace negative sentence instance aren't with are not\n",
    "def replace_not_dont(text):\n",
    "    if text.find(\"n't\"):\n",
    "        text = text.replace(\"n't\", \" not\")\n",
    "    if text.find(\"'s\"):\n",
    "        text = text.replace(\"'s\", \" is\")\n",
    "    if text.find(\"'d\"):\n",
    "        text = text.replace(\"'d\", \" would\")\n",
    "    if text.find(\"d've\"):\n",
    "        text = text.replace(\"d've\", \" would have\")\n",
    "    if text.find(\".\"):\n",
    "        text = text.replace(\".\", \" \")\n",
    "    if text.find(\",\"):\n",
    "        text = text.replace(\",\", \" \")\n",
    "    if text.find(\"'ll\"):\n",
    "        text = text.replace(\"'ll\", \" will\")\n",
    "    if text.find(\"'ve\"):\n",
    "        text = text.replace(\"'ve\", \" have\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    processing = os.path.join(path_file,'processing_data/train.dec')\n",
    "    lines=open(processing).read().split('\\n')\n",
    "    ht = dict()\n",
    "    conta = 0\n",
    "    for line in lines:\n",
    "        '''line = lower_case(line)\n",
    "        line = rid_number(line)\n",
    "        line = rid_space(line)\n",
    "        line = replace_not_dont(line)\n",
    "        line = rid_character(line)\n",
    "        _line = \" \".join(line.split())'''\n",
    "        for x in line.split(\" \"):\n",
    "            #x = rid_space(x)\n",
    "            if x not in ht and x!=\" \":\n",
    "                ht[x]=conta\n",
    "                conta+=1\n",
    "    return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add begin, end and space characters\n",
    "vocabulary['pad']=3482\n",
    "vocabulary['bos']=3483\n",
    "vocabulary['eos']=3484"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocabulary = {v:k for k,v in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(data_file):\n",
    "    if os.path.isfile(data_file):\n",
    "        with open(data_file, 'r') as read_file:\n",
    "            data = json.load(read_file)\n",
    "            return data\n",
    "\n",
    "def load_list_file(list_file):\n",
    "    with open(list_file, 'r') as read_file:\n",
    "        dialog_id_list = read_file.readlines()\n",
    "        dialog_id_list = [l.strip('\\n') for l in dialog_id_list]\n",
    "        return dialog_id_list\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_data_file = './data/MultiWOZ/data.json'\n",
    "dialog_data = load_json(dialog_data_file)\n",
    "dialog_id_list = list(set(dialog_data.keys())) # Bug: v1.0 contains duplicate id in the valid data\n",
    "print('# of dialogs:', len(dialog_data))\n",
    "#print(dialog_data['PMUL4641.json']) # print a sample dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_list_file = './data/MultiWOZ/valListFile.json'\n",
    "test_list_file = './data/MultiWOZ/testListFile.json'\n",
    "\n",
    "valid_id_list = list(set(load_list_file(valid_list_file)))\n",
    "test_id_list = load_list_file(test_list_file)\n",
    "train_id_list = [did for did in dialog_id_list if did not in (valid_id_list + test_id_list)]\n",
    "print('# of train dialogs:', len(train_id_list))\n",
    "print('# of valid dialogs:', len(valid_id_list))\n",
    "print('# of test dialogs :', len(test_id_list))\n",
    "assert(len(dialog_id_list) == len(train_id_list) + len(valid_id_list) + len(test_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [v for k, v in dialog_data.items() if k in train_id_list]\n",
    "valid_data = [v for k, v in dialog_data.items() if k in valid_id_list]\n",
    "test_data = [v for k, v in dialog_data.items() if k in test_id_list]\n",
    "assert(len(train_data) == len(train_id_list))\n",
    "assert(len(valid_data) == len(valid_id_list))\n",
    "assert(len(test_data) == len(test_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dst_diff(prev_d, crnt_d):\n",
    "    assert len(prev_d) == len(crnt_d)\n",
    "    diff = {}\n",
    "    for ((k1, v1), (k2, v2)) in zip(prev_d.items(), crnt_d.items()):\n",
    "        assert k1 == k2\n",
    "        if v1 != v2: # updated\n",
    "            diff[k2] = v2\n",
    "    return diff\n",
    "\n",
    "def analyze_dialog(train_data, print_dialog=True):\n",
    "    question = []\n",
    "    answer = []\n",
    "    for d in train_data:\n",
    "        if print_dialog:\n",
    "            prev_d = None\n",
    "            for i, t in enumerate(d['log']):\n",
    "                spk = 'Usr' if i % 2 == 0 else 'Sys' # Turn 0 is always a user's turn in this corpus.\n",
    "                if spk == 'Sys':\n",
    "                    if prev_d is None:\n",
    "                        prev_d = t['metadata']\n",
    "                    else:\n",
    "                        crnt_d = t['metadata']\n",
    "                        dst_diff = get_dst_diff(prev_d, crnt_d)\n",
    "                        #print('Updated DST:', dst_diff)\n",
    "                        prev_d = crnt_d\n",
    "                if i % 2 == 0:\n",
    "                    u = t['text']\n",
    "                    question.append(u)\n",
    "                else:\n",
    "                    u = t['text']\n",
    "                    answer.append(u)\n",
    "                #print('{}: {}'.format(spk, u))\n",
    "    return question, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,a = analyze_dialog(test_data, True)\n",
    "prepare_seq2seq_files(q,a,TESTSET_SIZE = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid():\n",
    "    x = os.path.join(path_file,'processing_data/train.enc')\n",
    "    y = os.path.join(path_file,'processing_data/train.dec')\n",
    "    x_ = os.path.join(path_file,'processing_data/valid.enc')\n",
    "    y_ = os.path.join(path_file,'processing_data/valid.dec')\n",
    "    X = []\n",
    "    Y = []\n",
    "    X_ = []\n",
    "    Y_ = []\n",
    "    lines=open(x).read().split('\\n')\n",
    "    for line in lines:\n",
    "        X.append(\"bos \" +line+\" eos\")\n",
    "    lines=open(y).read().split('\\n')\n",
    "    for line in lines:\n",
    "        Y.append(\"bos \" +line+\" eos\")\n",
    "    lines=open(x_).read().split('\\n')\n",
    "    for line in lines:\n",
    "        X_.append(\"bos \" +line+\" eos\")\n",
    "    lines=open(y_).read().split('\\n')\n",
    "    for line in lines:\n",
    "        Y_.append(\"bos \" +line+\" eos\")\n",
    "    return np.array(X),np.array(Y),np.array(X_),np.array(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y,X_,Y_=get_train_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_path=os.path.join(path_file,'peso/2.h5')\n",
    "file = h5py.File(hdf5_path, mode='r+')\n",
    "print(list(file.keys()))\n",
    "for a in file['embedding_1']:\n",
    "    print(a)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset data.h5\n",
    "hdf5_path=os.path.join(path_file,'data_h5/data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=h5py.File(hdf5_path,'w')\n",
    "x_r=file.create_group('X')\n",
    "y_r=file.create_group('Y')\n",
    "x_v=file.create_group('X_')\n",
    "y_v=file.create_group('Y_')\n",
    "yh_ta=file.create_group('Yh_train')\n",
    "yh_te=file.create_group('Yh_test') \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(hdf5_path, mode='r+')\n",
    "dataset=file['/Yh_train'].create_dataset('yh_train',(56779,63,3485), np.int8)\n",
    "#dataset=file['/Yh_test'].create_dataset('yh_test',(7375,63,50), np.int8)\n",
    "c = 0\n",
    "for x in range(50,56779,50): \n",
    "    dataset[c:x] = get_hot(Y[c:x], Ty = 63)\n",
    "    c=x\n",
    "dataset=file['/Yh_test'].create_dataset('yh_test',(7375,63,3485), np.int8)\n",
    "c = 0\n",
    "for x in range(50,7375,50): \n",
    "    dataset[c:x] = get_hot(Y_[c:x], Ty = 63)\n",
    "    c=x\n",
    "    \n",
    "dataset=file['/X'].create_dataset('x',X_train.shape, np.float64)\n",
    "dataset[...]=X_train\n",
    "\n",
    "dataset=file['/Y'].create_dataset('y',Y_train.shape, np.float64)\n",
    "dataset[...]=Y_train\n",
    "\n",
    "dataset=file['/X_'].create_dataset('x_',X_test.shape, np.float64)\n",
    "dataset[...]=X_test\n",
    "\n",
    "dataset=file['/Y_'].create_dataset('y_',Y_test.shape, np.float64)\n",
    "dataset[...]=Y_test\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Y:\n",
    "    l = len(i.split())\n",
    "    if  l > 63:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLenX = len(max(X, key=len).split())\n",
    "maxLenY = len(max(Y, key=len).split())\n",
    "maxLenX_ = len(max(X_, key=len).split())\n",
    "maxLenY_ = len(max(Y_, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} {} {} {}\".format(maxLenX,maxLenY,maxLenX_,maxLenY_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot(n, Ty):\n",
    "    Y=[]\n",
    "    vocab=n\n",
    "    for i in vocab:\n",
    "        string=i.split(' ')\n",
    "        rep = list(map(lambda x: vocabulary.get(x, 57), string))\n",
    "        if len(string) < Ty:\n",
    "            rep += [vocabulary['pad']] * (Ty - len(string))\n",
    "        Y.append(rep)\n",
    "    return np.array(list(map(lambda x: to_categorical(x, num_classes=3485), Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = get_hot(Y[:100], Ty = 63)\n",
    "Y_oh_test = get_hot(Y_[:200], Ty = 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file,encoding=\"utf-8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = os.path.join(path_file,'worc2vec/glove.6B.50d.txt')\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(embedings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word[138209.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contador = 0\n",
    "for x in file:\n",
    "    if x not in word_to_index:\n",
    "        print(x)\n",
    "        contador+= 1\n",
    "print(contador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contador = 0\n",
    "for x in file:\n",
    "    if x not in word_to_index:\n",
    "        print(x)\n",
    "        contador+= 1\n",
    "print(contador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_index['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(path_file,'data_h5/data.h5')\n",
    "dataset = h5py.File(path_dataset, \"r\")\n",
    "x=  np.array(dataset[\"/X/x\"][6])\n",
    "y=  np.array(dataset[\"/Y/y\"][1])\n",
    "yh= np.array(dataset[\"/Yh_train/yh_train\"][1])\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = yh.argmax(axis=-1)\n",
    "letra = list(map(lambda x:n_vocabulary[x],a))\n",
    "print(yh.argmax(axis=-1))\n",
    "print(x)\n",
    "print(index_to_word[185457.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataTrain(batch_size):\n",
    "    dataset = h5py.File(path_dataset, \"r\")\n",
    "    data_num=len(dataset['/X/x'])\n",
    "    batches_list = list(range(int(mt.ceil(float(data_num) / batch_size))))\n",
    "    while True:\n",
    "        for n, i in enumerate(batches_list):\n",
    "            i_s = i * batch_size  # index of the first image in this batch\n",
    "            i_e = min([(i + 1) * batch_size, data_num])  # index of the last image in this batch\n",
    "            x=  np.array(dataset[\"/X/x\"][i_s: i_e])\n",
    "            y=  np.array(dataset[\"/Y/y\"][i_s: i_e])\n",
    "            yh= np.array(dataset[\"/Yh_train/yh_train\"][i_s: i_e])\n",
    "            yield [x,y], yh  \n",
    "    dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataVal(batch_size):\n",
    "    dataset = h5py.File(path_dataset, \"r\")\n",
    "    data_num=len(dataset['/X_/x_'])\n",
    "    batches_list = list(range(int(mt.ceil(float(data_num) / batch_size))))\n",
    "    while True:\n",
    "        # loop over batches\n",
    "        for n, i in enumerate(batches_list):\n",
    "            i_s = i * batch_size  # index of the first image in this batch\n",
    "            i_e = min([(i + 1) * batch_size, data_num])  # index of the last image in this batch\n",
    "            x=  np.array(dataset[\"/X_/x_\"][i_s: i_e])\n",
    "            y=  np.array(dataset[\"/Y_/y_\"][i_s: i_e])\n",
    "            yh= np.array(dataset[\"/Yh_test/yh_test\"][i_s: i_e])\n",
    "            yield [x,y], yh\n",
    "    dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words =X[i].lower().split()\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j = j+1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len,emb_dim,trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sentences_to_indices(X, word_to_index, 63)\n",
    "Y_train = sentences_to_indices(Y, word_to_index, 63)\n",
    "X_test = sentences_to_indices(X_, word_to_index, 63)\n",
    "Y_test = sentences_to_indices(Y_, word_to_index, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(word_to_vec_map, \n",
    "              word_to_index,\n",
    "              step_input=63):\n",
    "    sentence_indices = Input(shape=(step_input,), dtype='int32')\n",
    "     # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    embed = Model(inputs = [sentence_indices], outputs = embeddings)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embed(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggg = embed.predict(Y_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = embed.predict(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model( word_to_vec_map, \n",
    "              word_to_index,\n",
    "              step_input=63,\n",
    "              step_out=63,\n",
    "              size_input_r=50,\n",
    "              output_dim=3485,\n",
    "              atten_units=64,\n",
    "              encoder_units=120,\n",
    "              decoder_units=240,\n",
    "              gmax= 60,\n",
    "              trainable=True,\n",
    "              return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    step_input -- length of the input sequence\n",
    "    step_out-- length of the output sequence\n",
    "    size_input_r -- dim input RGB image (CNN output)\n",
    "    size_input_d -- dim input depth image (CNN output)\n",
    "    size_input_j -- dim input skeleton image (CNN output)\n",
    "    output_dim -- size of the python dictionary \"lsp\"\n",
    "    atten_units -- number of units in attention (dense)\n",
    "    encoder_units -- hidden state in encoder\n",
    "    decoder_units -- hidden state in decoder\n",
    "    trainable -- boolean trainable or not trainable\n",
    "    return_probabilities -- boolean return attention probabilities\n",
    "    \n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    Builds a Neural Machine Translator that has alignment attention\n",
    "    \n",
    "    :return: keras.models.Model that can be compiled and fit'ed\n",
    "    *** REFERENCES ***\n",
    "    Lee, Jason, Kyunghyun Cho, and Thomas Hofmann. \n",
    "    \"Neural Machine Translation By Jointly Learning To Align and Translate\n",
    "    \" \n",
    "    \"\"\"\n",
    "            \n",
    "    sentence_indices_question = Input(shape=(step_input,), dtype='int32')\n",
    "     # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings_q = embedding_layer(sentence_indices_question)#(m, 40, 50)\n",
    "    # Step 2: encoder with two LSTM layers.\n",
    "    shortcut = Bidirectional(LSTM(encoder_units, return_sequences=True, dropout=0.5, recurrent_dropout=0.5),merge_mode='concat',trainable=trainable, name='BLSTM')(embeddings_q)\n",
    "    a= Bidirectional(LSTM(encoder_units, return_sequences=True, dropout=0.5, recurrent_dropout=0.5),merge_mode='concat',trainable=trainable, name='R_BLSTM')(shortcut)\n",
    "    a = Dropout(0.5, name='DROP_1')(a)\n",
    "    a = Add(name = 'ADD_1')([a,shortcut])\n",
    "    # Step 3: attention decoder.\n",
    "    sentence_indices_answer = Input(shape=(step_out,), dtype='int32')\n",
    "     # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings_a = embedding_layer(sentence_indices_answer)#(m, 63, 50)\n",
    "    \n",
    "    y_hat = attention_LSTM(decoder_units,\n",
    "                             steps=step_out,\n",
    "                             output_dim=output_dim,\n",
    "                             atten_units = atten_units,\n",
    "                             gmax = gmax,\n",
    "                             return_sequences=True,\n",
    "                             return_probabilities=return_probabilities,\n",
    "                             trainable=trainable,\n",
    "                             name = 'ATT_DEC')([a,embeddings_a])\n",
    "    \n",
    "    # Step 4: Create model instance taking three inputs and returning the list of outputs.\n",
    "    model = Model(inputs = [sentence_indices_question, sentence_indices_answer], outputs = y_hat)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model seq to seq (LSTM) with attention\n",
    "plot_model(model, to_file='chat.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01) # rgbdj\n",
    "#opt = RMSprop(lr=0.045, rho=0.94, epsilon=1.0, decay=0) #rgb\n",
    "early_stopping=EarlyStopping(monitor='val_loss', patience=6)\n",
    "mcp_save = ModelCheckpoint('chat.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "model.compile(optimizer=opt, metrics=['accuracy'], loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_j = np.expand_dims(X_j, axis=1)\n",
    "#X_j_ = np.expand_dims(X_j_, axis=1)\n",
    "# train model\n",
    "History=model.fit([X_train[:5000], Y_train[:5000]], Y_oh_train, epochs=2000,  callbacks=[early_stopping,mcp_save], batch_size=125, validation_data=([X_test[:200],Y_test[:200]],Y_oh_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(path_file,'data_h5/data.h5')\n",
    "tr_gen =  getDataTrain( batch_size=100 )\n",
    "va_gen =  getDataVal( batch_size=74 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "History=model.fit_generator(tr_gen, steps_per_epoch=568, epochs=1024, verbose=1,  callbacks=[early_stopping,mcp_save],  validation_data=va_gen, validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(History.history['loss'])\n",
    "plt.plot(History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation acc\n",
    "plt.plot(History.history['acc'])\n",
    "plt.plot(History.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['acc', 'val_acc'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save training history \n",
    "import pickle\n",
    "with open('chat_bot.pickle', 'wb') as file_pi:\n",
    "        pickle.dump(History.history, file_pi, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss\n",
    "#plt.figure(figsize=(14, 10))\n",
    "plt.plot(History.history['loss'])\n",
    "plt.plot(History.history['val_loss'])\n",
    "plt.title('Model - Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.savefig(os.path.join(path_file,'validation_loss.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(History.history['acc'])\n",
    "plt.plot(History.history['val_acc'])\n",
    "plt.title('Model - Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.savefig(os.path.join(path_file,'validation_acc.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('chat_bot_wight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = os.path.join(path_file,'peso/2.h5')\n",
    "model.load_weights(WEIGHTS_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = embedding_layer.get_weights()\n",
    "print(np.array(aa).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].set_weights(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict( sent ):\n",
    "    words = sent.split(' ')\n",
    "    words = ['bos'] + words + ['eos']\n",
    "    words_id = []\n",
    "    for w in words:\n",
    "        if w in word_to_index:\n",
    "            words_id.append( word_to_index[w] )\n",
    "        else:\n",
    "            words_id.append( word_to_index['unk'] )\n",
    "    words = words_id\n",
    "\n",
    "\n",
    "    ret = \"\"\n",
    "    print(words)\n",
    "    m_input = [ np.zeros((1,63)) , np.zeros((1,63)) ]\n",
    "    for i , w in enumerate( words ):\n",
    "        m_input[0][0 , i ] = w\n",
    "    m_input[1][0,0] = word_to_index['bos']\n",
    "    for w_i in range(1,63):\n",
    "        out = model.predict( m_input )\n",
    "        \n",
    "        out_w_i = np.argmax(out[0][w_i-1])\n",
    "        if out_w_i == 0:\n",
    "            continue\n",
    "        ret +=  n_vocabulary[out_w_i] + \" \"\n",
    "        out_w_i = word_to_index[n_vocabulary[out_w_i]]\n",
    "        m_input[1][0,w_i] = out_w_i\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    sent = input(\"Enter a sentence : \")\n",
    "    print (predict( sent ).encode('utf-8'))\n",
    "\n",
    "    print (\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
